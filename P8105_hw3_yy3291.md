Homework 3
================
Youssra Yemmas
2023-10-14

# Problem 1

``` r
data("instacart")
summary(instacart)
```

    ##     order_id         product_id    add_to_cart_order   reordered     
    ##  Min.   :      1   Min.   :    1   Min.   : 1.000    Min.   :0.0000  
    ##  1st Qu.: 843370   1st Qu.:13380   1st Qu.: 3.000    1st Qu.:0.0000  
    ##  Median :1701880   Median :25298   Median : 7.000    Median :1.0000  
    ##  Mean   :1706298   Mean   :25556   Mean   : 8.758    Mean   :0.5986  
    ##  3rd Qu.:2568023   3rd Qu.:37940   3rd Qu.:12.000    3rd Qu.:1.0000  
    ##  Max.   :3421070   Max.   :49688   Max.   :80.000    Max.   :1.0000  
    ##     user_id         eval_set          order_number      order_dow    
    ##  Min.   :     1   Length:1384617     Min.   :  4.00   Min.   :0.000  
    ##  1st Qu.: 51732   Class :character   1st Qu.:  6.00   1st Qu.:1.000  
    ##  Median :102933   Mode  :character   Median : 11.00   Median :3.000  
    ##  Mean   :103113                      Mean   : 17.09   Mean   :2.701  
    ##  3rd Qu.:154959                      3rd Qu.: 21.00   3rd Qu.:5.000  
    ##  Max.   :206209                      Max.   :100.00   Max.   :6.000  
    ##  order_hour_of_day days_since_prior_order product_name          aisle_id    
    ##  Min.   : 0.00     Min.   : 0.00          Length:1384617     Min.   :  1.0  
    ##  1st Qu.:10.00     1st Qu.: 7.00          Class :character   1st Qu.: 31.0  
    ##  Median :14.00     Median :15.00          Mode  :character   Median : 83.0  
    ##  Mean   :13.58     Mean   :17.07                             Mean   : 71.3  
    ##  3rd Qu.:17.00     3rd Qu.:30.00                             3rd Qu.:107.0  
    ##  Max.   :23.00     Max.   :30.00                             Max.   :134.0  
    ##  department_id      aisle            department       
    ##  Min.   : 1.00   Length:1384617     Length:1384617    
    ##  1st Qu.: 4.00   Class :character   Class :character  
    ##  Median : 8.00   Mode  :character   Mode  :character  
    ##  Mean   : 9.84                                        
    ##  3rd Qu.:16.00                                        
    ##  Max.   :21.00

``` r
length(instacart)
```

    ## [1] 15

``` r
nrow(instacart)
```

    ## [1] 1384617

``` r
ncol(instacart)
```

    ## [1] 15

From an initial summarizing look at the data instacart there are
1,384,617 observations of 15 variables with 4 variables that are
categorical and 11 that are numerical. There also seems to be one binary
variable, reordered, that corresponds to if an item is ordered again or
not. Ther variables order day of the week and order hour of the day are
interesting and show a median of 3 for day of the week meaning Wednesday
is a day clients most often place their orders and with a median of 14
for hour of the day it seems most orders are places at 2 pm.

# Problem 1 cont.-how many aisles are there and which aisles are the most items ordered from

``` r
instacart |> 
  count(aisle) |> 
  arrange(desc(n))
```

    ## # A tibble: 134 × 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # ℹ 124 more rows

``` r
instacart1_df =
instacart %>% 
  janitor::clean_names() %>% 
  group_by(aisle_id, aisle) %>% 
  summarize(n_obs = n())
```

    ## `summarise()` has grouped output by 'aisle_id'. You can override using the
    ## `.groups` argument.

We can see from the subsetted data frame that there are 134 aisles and
using the toggle arrows when viewing the instacart1_df we can see that
some of the aisles where most items are ordered from are the fresh
vegetables, fresh fruits, packaged vegetables fruits, yoghurt and
packaged cheese aisles.

# Making a plot with number of items ordered in each aisle, filtering to aisles with more than 10000 items ordered

``` r
instacart |> 
  count(aisle) |> 
  filter(n > 10000) |> 
  mutate(aisle = fct_reorder(aisle, n)) |> 
  ggplot(aes(x = aisle, y = n)) + 
  geom_point(aes(color = aisle)) + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```

<img src="P8105_hw3_yy3291_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

``` r
instacart1_df %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle_id = as.factor(aisle),
    aisle_id = fct_reorder(aisle, n)
  )
```

    ## # A tibble: 0 × 3
    ## # Groups:   aisle_id [0]
    ## # ℹ 3 variables: aisle_id <fct>, aisle <chr>, n <int>

``` r
aisles_plot =
instacart1_df %>% 
  ggplot(aes(x = aisle_id, y = n_obs)) +
  geom_histogram(stat = "identity") +
  labs(title = "Number of Items Ordered in Each Aisle", x = "Name of Aisles", y = "Number of Items Ordered")
```

    ## Warning in geom_histogram(stat = "identity"): Ignoring unknown parameters:
    ## `binwidth`, `bins`, and `pad`

``` r
aisles_plot
```

<img src="P8105_hw3_yy3291_files/figure-gfm/unnamed-chunk-3-2.png" width="90%" />

# Making a table showing the most popular items in each of the aisles baking ingredients, dog food care, and packaged vegetables fruits including the number of times each is ordered

``` r
instacart |> 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) |>
  group_by(aisle) |> 
  count(product_name) |> 
  mutate(rank = min_rank(desc(n))) |> 
  filter(rank < 4) |> 
  arrange(desc(n)) |>
  knitr::kable()
```

| aisle                      | product_name                                  |    n | rank |
|:---------------------------|:----------------------------------------------|-----:|-----:|
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |

This creates a data frame where we can toggle through and see that in
the aisle baking ingredients the two most ordered items are Light Brown
Sugar which was ordered 157 times and Pure Baking Soda which was ordered
140 times; in the aisle dog food care the two most ordered items in dog
food care are Organix Grain Free Chicken & Vegetable Dog Food which was
ordered 14 times and Organix Chicken & Brown Rice Recipe whcih was
ordered 13 times and lastly in packaged vegetables fruits the two most
ordered items are Organic Baby Spinach which was ordered 3324 times and
Organic Raspberries which was ordered 1920 times.

# Making a data frame showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week

``` r
mean_hour_df = 
  instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable(digits = 2)
```

    ## `summarise()` has grouped output by 'product_name'. You can override using the
    ## `.groups` argument.

From the data we can see that the mean hour of the day at which the
items Pink Lady Apples and Coffee Ice Cream are ordered each day of the
week.

# Problem 2

### First will need to focus on Overall Health Topic, including only responses from Excellent to Poor and organize the responses as a factor taking levels ordered from Poor to Excellent

``` r
data("brfss_smart2010")
summary(brfss_smart2010)
```

    ##       Year      Locationabbr       Locationdesc          Class          
    ##  Min.   :2002   Length:134203      Length:134203      Length:134203     
    ##  1st Qu.:2005   Class :character   Class :character   Class :character  
    ##  Median :2007   Mode  :character   Mode  :character   Mode  :character  
    ##  Mean   :2007                                                           
    ##  3rd Qu.:2009                                                           
    ##  Max.   :2010                                                           
    ##                                                                         
    ##     Topic             Question           Response          Sample_Size    
    ##  Length:134203      Length:134203      Length:134203      Min.   :   1.0  
    ##  Class :character   Class :character   Class :character   1st Qu.:  59.0  
    ##  Mode  :character   Mode  :character   Mode  :character   Median : 153.0  
    ##                                                           Mean   : 271.7  
    ##                                                           3rd Qu.: 353.0  
    ##                                                           Max.   :4505.0  
    ##                                                           NA's   :2       
    ##    Data_value    Confidence_limit_Low Confidence_limit_High Display_order 
    ##  Min.   : 0.10   Min.   : 0.00        Min.   :  0.00        Min.   : 1.0  
    ##  1st Qu.:14.30   1st Qu.: 8.50        1st Qu.: 16.30        1st Qu.:16.0  
    ##  Median :33.20   Median :25.90        Median : 37.00        Median :32.0  
    ##  Mean   :43.23   Mean   :37.74        Mean   : 45.76        Mean   :32.4  
    ##  3rd Qu.:77.30   3rd Qu.:71.10        3rd Qu.: 81.55        3rd Qu.:48.0  
    ##  Max.   :99.90   Max.   :99.70        Max.   :100.00        Max.   :68.0  
    ##  NA's   :4515                                                             
    ##  Data_value_unit    Data_value_type    Data_Value_Footnote_Symbol
    ##  Length:134203      Length:134203      Length:134203             
    ##  Class :character   Class :character   Class :character          
    ##  Mode  :character   Mode  :character   Mode  :character          
    ##                                                                  
    ##                                                                  
    ##                                                                  
    ##                                                                  
    ##  Data_Value_Footnote  DataSource          ClassId            TopicId         
    ##  Length:134203       Length:134203      Length:134203      Length:134203     
    ##  Class :character    Class :character   Class :character   Class :character  
    ##  Mode  :character    Mode  :character   Mode  :character   Mode  :character  
    ##                                                                              
    ##                                                                              
    ##                                                                              
    ##                                                                              
    ##   LocationID         QuestionID           RESPID          GeoLocation       
    ##  Length:134203      Length:134203      Length:134203      Length:134203     
    ##  Class :character   Class :character   Class :character   Class :character  
    ##  Mode  :character   Mode  :character   Mode  :character   Mode  :character  
    ##                                                                             
    ##                                                                             
    ##                                                                             
    ## 

``` r
janitor::clean_names(brfss_smart2010)
```

    ## # A tibble: 134,203 × 23
    ##     year locationabbr locationdesc     class topic question response sample_size
    ##    <int> <chr>        <chr>            <chr> <chr> <chr>    <chr>          <int>
    ##  1  2010 AL           AL - Jefferson … Heal… Over… How is … Excelle…          94
    ##  2  2010 AL           AL - Jefferson … Heal… Over… How is … Very go…         148
    ##  3  2010 AL           AL - Jefferson … Heal… Over… How is … Good             208
    ##  4  2010 AL           AL - Jefferson … Heal… Over… How is … Fair             107
    ##  5  2010 AL           AL - Jefferson … Heal… Over… How is … Poor              45
    ##  6  2010 AL           AL - Jefferson … Heal… Fair… Health … Good or…         450
    ##  7  2010 AL           AL - Jefferson … Heal… Fair… Health … Fair or…         152
    ##  8  2010 AL           AL - Jefferson … Heal… Heal… Do you … Yes              524
    ##  9  2010 AL           AL - Jefferson … Heal… Heal… Do you … No                77
    ## 10  2010 AL           AL - Jefferson … Heal… Unde… Adults … Yes              316
    ## # ℹ 134,193 more rows
    ## # ℹ 15 more variables: data_value <dbl>, confidence_limit_low <dbl>,
    ## #   confidence_limit_high <dbl>, display_order <int>, data_value_unit <chr>,
    ## #   data_value_type <chr>, data_value_footnote_symbol <chr>,
    ## #   data_value_footnote <chr>, data_source <chr>, class_id <chr>,
    ## #   topic_id <chr>, location_id <chr>, question_id <chr>, respid <chr>,
    ## #   geo_location <chr>

``` r
brfss_smart2010_df =
brfss_smart2010 %>% 
  filter(Topic %in% c("Overall Health")) %>% 
  group_by(Response) %>% 
  mutate(
    Response = factor(Response, levels = c("Poor", "Fair", "Good", "Very Good", "Excellent"))
  ) %>% 
  arrange(Response)
```

### In 2002, which states were observed at 7 or more locations? What about in 2010?

``` r
location_2002_df = 
  brfss_smart2010_df %>% 
  filter(Year == 2002) %>% 
  group_by(Locationabbr) %>% 
  summarize(
    n_obs = n(),
    n_locations = n_distinct(Locationdesc)
  ) %>% 
  filter(n_locations >= 7) %>% 
  select(Locationabbr)
 knitr::kable(location_2002_df)
```

| Locationabbr |
|:-------------|
| CT           |
| FL           |
| MA           |
| NC           |
| NJ           |
| PA           |

``` r
# We can see that in 2002 the states Connecticut, Florida, Massachusetts, North Carolina, New Jersey and Pennsylvania were observed at 7 or mopre locations.
 
location_2010_df = 
  brfss_smart2010_df %>% 
  filter(Year == 2010) %>% 
  group_by(Locationabbr) %>% 
  summarize(
    n_obs = n(),
    n_locations = n_distinct(Locationdesc)
  ) %>% 
  filter(n_locations >= 7) %>% 
  select(Locationabbr)
 knitr::kable(location_2010_df)
```

| Locationabbr |
|:-------------|
| CA           |
| CO           |
| FL           |
| MA           |
| MD           |
| NC           |
| NE           |
| NJ           |
| NY           |
| OH           |
| PA           |
| SC           |
| TX           |
| WA           |

``` r
# We can see that in 2010 the states California, Colorado, Florida, Massachusetts, Maryland, North Carolina, Nebraska, New Jersey, New York, Ohio, Pennsylvania, South Carolina, Texas and Washington were observed at 7 or mopre locations.
```

# Constructing a dataset that is limited to Excellent responses, and contains, year, state, and a variable that averages the data_value across locations within a state. Make a “spaghetti” plot of this average value over time within a state (that is, make a plot showing a line for each state across years – the geom_line geometry and group aesthetic will help).

``` r
excellent_df = 
  brfss_smart2010_df %>% 
  filter(Response == "Excellent") %>% 
  select(Year, Locationabbr, Data_value) %>% 
  group_by(Locationabbr, Year) %>% 
  mutate(
    mean_data_value = mean(Data_value, na.rm = TRUE)
  )
```

    ## Adding missing grouping variables: `Response`

``` r
excellent_plot =
  ggplot(excellent_df, aes(x = Year, y = mean_data_value, group = Locationabbr, color = Locationabbr)) +
  geom_line() +
  labs(title = "Spaghetti Plot of Mean Data Value Within Each State", x = "Year", Y = "Mean Values", color = "State")
excellent_plot
```

<img src="P8105_hw3_yy3291_files/figure-gfm/unnamed-chunk-8-1.png" width="90%" />

# Making a two-panel plot showing, for the years 2006, and 2010, distribution of data_value for responses (“Poor” to “Excellent”) among locations in NY State.

``` r
nystate_2006_df = 
  brfss_smart2010_df %>% 
  filter(Locationabbr == "NY", Year == "2006") %>% 
  drop_na(Response)

nystate_2010_df = 
  brfss_smart2010_df %>% 
  filter(Locationabbr == "NY", Year == "2010") %>% 
  drop_na(Response)

nystate_0610_df =
  full_join(nystate_2006_df, nystate_2010_df)
```

    ## Joining with `by = join_by(Year, Locationabbr, Locationdesc, Class, Topic,
    ## Question, Response, Sample_Size, Data_value, Confidence_limit_Low,
    ## Confidence_limit_High, Display_order, Data_value_unit, Data_value_type,
    ## Data_Value_Footnote_Symbol, Data_Value_Footnote, DataSource, ClassId, TopicId,
    ## LocationID, QuestionID, RESPID, GeoLocation)`

``` r
nystate0610_plot = 
  ggplot(nystate_0610_df, aes(x = Response, y = Data_value, color = Locationabbr)) +
  geom_point() 
nystate0610_plot + facet_grid(Response ~ Year)
```

<img src="P8105_hw3_yy3291_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />
